# Fake-data Simulation

## "Hello World" example

We demonstrate fake-data simulation with the linear regression example from the previous chapter.  Here is the Stan program:

```{r}
print_file("stan/simplest-regression.stan")
```

### Simulating fake data conditional on assumed parameter values

We need to construct the data:  N, x, and y.  But to do this we must also pick assumed values for the parameters:  a, b, sigma.

We do this as follows:
- Set N = 100
- Set a = 2
- Set b = 3
- Set sigma = 5
- Sample x as N independent values uniformly distributed between 0 and 10
- Sample y_n, n=1,...,N independently with mean a + b*x and standard deviation sigma.

These steps can all be done in R as in the linked code.

```{r echo=FALSE}
N <- 100
a <- 2
b <- 3
sigma <- 5
x <- runif(N, 0, 10)
y <- rnorm(N, a + b*x, sigma)
hello_data <- list(N=N, x=x, y=y)
```

### Fitting the Stan model and reading the output

We can now fit the Stan model, passing in the data list N, x, y.

```{r fit-data-cmd, echo=FALSE, eval=FALSE}
fit <- stan("stan/simplest-regression.stan", data = hello_data)
```
```{r fit-data-do, include=FALSE}
fit <- stan("stan/simplest-regression.stan", data = hello_data)
```

Here is the summary of the fitted model:

```{r fit-data}
print(fit)
```

(The below description will all change as we are changing default Stan output.)

Now we go through the output:

* The first few lines summarize the Stan run, with the name of the
  file, the number of chains and iterations.  In this case, Stan ran
  the default 4 chains with 1000 warmup iterations followed by 1000
  post-warmup iterations, yielding 4000 post-warmup simulation draws
  in total.

* The left column of the table has the names of parameters,
  transformed parameters, and generated quantities produced by
  `model.stan`.  In this case, the parameters are a, b, and sigma; the
  only transformed parameter is `lp__` (the log-posterior density or
  target function created by the Stan model); and there are no
  generated quantities.

* The next column of the table shows the mean (average) of the 4000
  draws for each quantity.

* The next column shows the Monte Carlo standard error, which is an
  estimate of the uncertainty in the mean.

* The next column shows the standard deviation of the draws for each
  quantity.  As the number of simulation draws increases, mean should
  approach the posterior mean, se_mean should go to zero, and sd
  should approach the posterior standard deviation.  For most purposes
  we can ignore se_mean.

* The next several columns give quantiles of the simulations.

* The next columns gives the effective sample size and
  $\widehat{R}$. Typically we want $\widehat{R}$ to be less then 1.1
  for each row of the table.

In the above output, $\widehat{R}$ is less then 1.1 for all
quantities, so the chains seem to have mixed well, and we use the
results to summarize the posterior distribution.

### Comparing fitted model to assumed parameter values

We can compare the
posterior inferences to the assumed parameter values (here, $a=`r a`$, $b=`r b`$,
and $\sigma=`r sigma`$).  These assumed values are roughly within the range of
uncertainty of the inferences.

## Prior predictive simulation

Do an example here

## Topics in fake-data simulation

We will expand upon these topics:

- Choosing assumed parameter values (as in above example) or drawing parameters from the prior

- When choosing assumed parameter values can give a bad answer (if they're in the tail of the prior)

- Fake-data simulation for hierarchical models:  You can choose assumed values for the hyperparameters but you should draw the lower-level parameters from the model

- Choosing or drawing random values for unmodeled data such as N and x

- What if fake-data simulation gives a bad answer, if the model cannot recover the assumed parameter values?